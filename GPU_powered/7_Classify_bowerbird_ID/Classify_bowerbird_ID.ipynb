{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  20%|██        | 1/5 [00:09<00:38,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video B18_20181113_089.MP4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  40%|████      | 2/5 [00:14<00:20,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video B18_20181114_005.MP4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  60%|██████    | 3/5 [00:19<00:11,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video B18_20181201_056.MP4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  80%|████████  | 4/5 [00:29<00:07,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video B18_20181208_005.MP4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 5/5 [00:34<00:00,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video B18_20181210_007.MP4\n",
      "Extracted frames saved in: D:\\Bowerbird-ID\\7_Classify_bowerdbird_ID\\Extracted_frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "INPUT_DIR = r\"D:\\Bowerbird-ID\\7_Classify_bowerdbird_ID\\Videos_to_classify\" \n",
    "OUTPUT_DIR = r\"D:\\Bowerbird-ID\\7_Classify_bowerdbird_ID\\Extracted_frames\" \n",
    "YOLO_MODEL_PATH = \"yolo11x-seg.pt\" \n",
    "SAMPLING_INTERVAL = 60  # Extract a frame every X frames\n",
    "IOU_THRESHOLD = 0.5  # threshold for filtering overlapping detections\n",
    "SIMILARITY_THRESHOLD = 5  # pHash similarity threshold\n",
    "\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH) # Load model\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculates Intersection over Union (IoU) >1 bounding boxes\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2\n",
    "\n",
    "    inter_x1 = max(x1, x1_)\n",
    "    inter_y1 = max(y1, y1_)\n",
    "    inter_x2 = min(x2, x2_)\n",
    "    inter_y2 = min(y2, y2_)\n",
    "\n",
    "    inter_area = max(0, inter_x2 - inter_x1 + 1) * max(0, inter_y2 - inter_y1 + 1)\n",
    "    box1_area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    box2_area = (x2_ - x1_ + 1) * (y2_ - y1_ + 1)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frame_count = 0\n",
    "    unique_hashes = []\n",
    "    multi_bird_detected = False\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % SAMPLING_INTERVAL == 0:\n",
    "            results = yolo_model.predict(frame, conf=0.6, verbose=False)\n",
    "            detections = results[0].boxes\n",
    "\n",
    "            if len(detections) > 0:\n",
    "                # Filter detections based on IoU. Take the highest confidence bbox\n",
    "                filtered_detections = []\n",
    "                for i, box in enumerate(detections.xyxy):\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    score = detections.conf[i]\n",
    "                    if all(calculate_iou((x1, y1, x2, y2), det[:4]) <= IOU_THRESHOLD for det in filtered_detections):\n",
    "                        filtered_detections.append((x1, y1, x2, y2, score))\n",
    "\n",
    "                if len(filtered_detections) > 1:\n",
    "                    multi_bird_detected = True\n",
    "                    break\n",
    "\n",
    "                x1, y1, x2, y2, _ = max(filtered_detections, key=lambda d: d[-1])\n",
    "                cropped = frame[y1:y2, x1:x2]\n",
    "\n",
    "                pil_image = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "                frame_hash = imagehash.phash(pil_image)\n",
    "\n",
    "                if all(abs(frame_hash - h) > SIMILARITY_THRESHOLD for h in unique_hashes):\n",
    "                    frame_name = f\"{video_path.stem}_frame{frame_count}.png\"\n",
    "                    frame_path = os.path.join(OUTPUT_DIR, frame_name)\n",
    "                    cv2.imwrite(frame_path, frame)\n",
    "                    unique_hashes.append(frame_hash)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if multi_bird_detected:\n",
    "        print(f\"There is more than one bird in the video {video_path.name}, choose another one!\")\n",
    "    else:\n",
    "        print(f\"Processed video {video_path.name}\")\n",
    "\n",
    "video_files = list(Path(INPUT_DIR).glob(\"*.MP4\"))\n",
    "if not video_files:\n",
    "    print(\"No videos in the input directory\")\n",
    "else:\n",
    "    for video in tqdm(video_files, desc=\"Processing videos\"):\n",
    "        process_video(video)\n",
    "\n",
    "print(f\"Extracted frames saved in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Frames: 100%|██████████| 68/68 [00:17<00:00,  3.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from scipy.ndimage import label\n",
    "\n",
    "input_dir = \"extracted_frames\"\n",
    "yolo_model = YOLO('yolo11x-seg.pt') \n",
    "MIN_BLOB_PIXELS = 5000\n",
    "BOTTOM_FRACTION_ROW = 1 / 4\n",
    "BOTTOM_FRACTION_NARROW = 1 / 2\n",
    "HORIZONTAL_THRESHOLD = 0.8\n",
    "NARROW_SEGMENT_THRESHOLD = 100\n",
    "\n",
    "def filter_horizontal_rows(mask, threshold, bottom_fraction):\n",
    "    start_row = int(mask.shape[0] * (1 - bottom_fraction))\n",
    "    for row_idx in range(start_row, mask.shape[0]):\n",
    "        row = mask[row_idx, :]\n",
    "        if 1 - (np.sum(row) / row.shape[0]) >= threshold:\n",
    "            mask[row_idx, :] = 0\n",
    "    return mask\n",
    "\n",
    "def filter_narrow_segments(mask, max_width, bottom_fraction):\n",
    "    start_row = int(mask.shape[0] * (1 - bottom_fraction))\n",
    "    for row_idx in range(start_row, mask.shape[0]):\n",
    "        segments = np.split(np.where(mask[row_idx])[0], np.where(np.diff(np.where(mask[row_idx])[0]) > 1)[0] + 1)\n",
    "        for segment in segments:\n",
    "            if len(segment) <= max_width:\n",
    "                mask[row_idx, segment] = 0\n",
    "    return mask\n",
    "\n",
    "def remove_small_blobs(mask, min_pixels):\n",
    "    labeled_mask, num_features = label(mask)\n",
    "    return np.isin(labeled_mask, [i for i in range(1, num_features + 1) if np.sum(labeled_mask == i) >= min_pixels])\n",
    "\n",
    "for frame_name in tqdm(os.listdir(input_dir), desc=\"Frames\"):\n",
    "    frame_path = os.path.join(input_dir, frame_name)\n",
    "    if not frame_name.lower().endswith('.png'):\n",
    "        continue\n",
    "\n",
    "    # YOLO detection\n",
    "    results = yolo_model.predict(frame_path, conf=0.4, verbose=False)\n",
    "    if not results[0].boxes:\n",
    "        os.remove(frame_path)\n",
    "        continue\n",
    "\n",
    "    # Crop region of interest\n",
    "    img = cv2.imread(frame_path)\n",
    "    x1, y1, x2, y2 = map(int, results[0].boxes.xyxy[0])  # Highest-confidence box\n",
    "    cropped = img[y1:y2, x1:x2]\n",
    "\n",
    "    # Mask prediction\n",
    "    mask_results = yolo_model.predict(cropped, conf=0.6, verbose=False)\n",
    "    if not mask_results[0].masks:\n",
    "        os.remove(frame_path)\n",
    "        continue\n",
    "\n",
    "    mask = mask_results[0].masks.data[0].cpu().numpy().astype(bool)\n",
    "\n",
    "    # Apply filters\n",
    "    mask = filter_horizontal_rows(mask, HORIZONTAL_THRESHOLD, BOTTOM_FRACTION_ROW)\n",
    "    mask = filter_narrow_segments(mask, NARROW_SEGMENT_THRESHOLD, BOTTOM_FRACTION_NARROW)\n",
    "    mask = remove_small_blobs(mask, MIN_BLOB_PIXELS)\n",
    "\n",
    "    if not np.any(mask):  # Remove file if no valid mask\n",
    "        os.remove(frame_path)\n",
    "        continue\n",
    "\n",
    "    # Resize mask to match cropped dimensions if necessary\n",
    "    if mask.shape[:2] != cropped.shape[:2]:\n",
    "        mask = cv2.resize(mask.astype(np.uint8), (cropped.shape[1], cropped.shape[0]),\n",
    "                      interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    # Apply mask and save\n",
    "    mask_rgb = np.zeros_like(cropped)\n",
    "    mask_rgb[mask] = cropped[mask]\n",
    "    cv2.imwrite(frame_path, mask_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Athena\\miniconda3\\envs\\bowerbird\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Athena\\miniconda3\\envs\\bowerbird\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Processing frames: 100%|██████████| 42/42 [00:00<00:00, 44.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Results:\n",
      "B26: 40.48%\n",
      "B31: 35.71%\n",
      "B50: 9.52%\n",
      "B07: 7.14%\n",
      "B47: 4.76%\n",
      "B11: 2.38%\n",
      "\n",
      "The bird is most likely B26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_PATH = r\"D:\\Bowerbird-ID\\6_Train_ResNet50_classifier\\best_model.pth\"\n",
    "FRAME_DIR = \"extracted_frames\"  \n",
    "CLASS_NAMES = ['B02', 'B03', 'B04', 'B05', 'B07', 'B11', 'B18', 'B23', 'B26', 'B29', 'B30', 'B31', 'B47', 'B49', 'B50', 'B52']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=False, num_classes=16)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(CLASS_NAMES))\n",
    "\n",
    "# Move the model to the device and set it to evaluation mode (ERROR)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalisation\n",
    "])\n",
    "\n",
    "predictions = []\n",
    "for frame_name in tqdm(os.listdir(FRAME_DIR), desc=\"Processing frames\"):\n",
    "    frame_path = os.path.join(FRAME_DIR, frame_name)\n",
    "    if not frame_name.lower().endswith('.png'):\n",
    "        continue\n",
    "\n",
    "    image = Image.open(frame_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "        predicted_class = probabilities.argmax().item()\n",
    "\n",
    "        if predicted_class >= len(CLASS_NAMES):\n",
    "            print(f\"Warning: Predicted class index {predicted_class} is out of bounds for CLASS_NAMES.\")\n",
    "            continue\n",
    "\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "# Count predictions for each class and calculate percentages\n",
    "prediction_counts = Counter(predictions)\n",
    "total_predictions = sum(prediction_counts.values())\n",
    "\n",
    "if total_predictions == 0:\n",
    "    print(\"No valid predictions were made.\")\n",
    "    exit()\n",
    "\n",
    "percentages = {CLASS_NAMES[k]: (v / total_predictions) * 100 for k, v in prediction_counts.items()}\n",
    "sorted_percentages = dict(sorted(percentages.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Determine the most common class\n",
    "most_common_class = max(prediction_counts, key=prediction_counts.get)\n",
    "most_common_percentage = sorted_percentages[CLASS_NAMES[most_common_class]]\n",
    "\n",
    "# Print results\n",
    "print(\"\\nPrediction Results:\")\n",
    "for bird_id, percentage in sorted_percentages.items():\n",
    "    print(f\"{bird_id}: {percentage:.2f}%\")\n",
    "\n",
    "print(f\"\\nThe bird is most likely {CLASS_NAMES[most_common_class]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
