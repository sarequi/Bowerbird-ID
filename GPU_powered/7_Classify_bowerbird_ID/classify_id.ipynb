{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sarah/Bowerbird-ID/GPU_powered/7_Classify_bowerbird_ID'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  20%|██        | 1/5 [01:13<04:54, 73.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video B18_20181201_056.MP4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  40%|████      | 2/5 [02:38<04:01, 80.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video B18_20181113_089.MP4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  60%|██████    | 3/5 [03:49<02:31, 76.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video B18_20181114_005.MP4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  80%|████████  | 4/5 [05:00<01:13, 73.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video B18_20181210_007.MP4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 5/5 [07:00<00:00, 84.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed video B18_20181208_005.MP4\n",
      "Extracted frames saved in: Extracted_frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Frames: 100%|██████████| 68/68 [10:42<00:00,  9.45s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from scipy.ndimage import label\n",
    "\n",
    "# Frame Extraction\n",
    "\n",
    "INPUT_DIR = \"Videos_to_classify\"\n",
    "OUTPUT_DIR = \"Extracted_frames\"\n",
    "YOLO_MODEL_PATH = \"yolo11x-seg.pt\"\n",
    "SAMPLING_INTERVAL = 60  # Extract a frame every X frames\n",
    "IOU_THRESHOLD = 0.5     # Threshold for filtering overlapping detections\n",
    "SIMILARITY_THRESHOLD = 5  # pHash similarity threshold\n",
    "\n",
    "yolo_model = YOLO(YOLO_MODEL_PATH)  # Load model\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculates Intersection over Union (IoU)\"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2  \n",
    "\n",
    "    inter_x1 = max(x1, x1_)\n",
    "    inter_y1 = max(y1, y1_)\n",
    "    inter_x2 = min(x2, x2_)\n",
    "    inter_y2 = min(y2, y2_)\n",
    "\n",
    "    inter_area = max(0, inter_x2 - inter_x1 + 1) * max(0, inter_y2 - inter_y1 + 1)\n",
    "    box1_area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    box2_area = (x2_ - x1_ + 1) * (y2_ - y1_ + 1)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frame_count = 0\n",
    "    unique_hashes = []\n",
    "    multi_bird_detected = False\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Extract frames\n",
    "        if frame_count % SAMPLING_INTERVAL == 0:\n",
    "            results = yolo_model.predict(frame, conf=0.6, verbose=False)\n",
    "            detections = results[0].boxes\n",
    "\n",
    "            if len(detections) > 0:\n",
    "                # Filter detections based on IoU --> pick the highest-confidence bbox\n",
    "                filtered_detections = []\n",
    "                for i, box in enumerate(detections.xyxy):\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    score = detections.conf[i]\n",
    "                    # Only add if IoU with existing detections is below threshold\n",
    "                    if all(calculate_iou((x1, y1, x2, y2), det[:4]) <= IOU_THRESHOLD for det in filtered_detections):\n",
    "                        filtered_detections.append((x1, y1, x2, y2, score))\n",
    "\n",
    "                # If more than one detection remains, we suspect there are multiple birds in the frame\n",
    "                if len(filtered_detections) > 1:\n",
    "                    multi_bird_detected = True\n",
    "                    break\n",
    "\n",
    "                # Keep the detection with the highest confidence\n",
    "                x1, y1, x2, y2, _ = max(filtered_detections, key=lambda d: d[-1])\n",
    "                cropped = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # Compute a perceptual hash of the cropped image\n",
    "                pil_image = Image.fromarray(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "                frame_hash = imagehash.phash(pil_image)\n",
    "\n",
    "                # Save frame only if it is not too similar to previous frames\n",
    "                if all(abs(frame_hash - h) > SIMILARITY_THRESHOLD for h in unique_hashes):\n",
    "                    frame_name = f\"{video_path.stem}_frame{frame_count}.png\"\n",
    "                    frame_path = os.path.join(OUTPUT_DIR, frame_name)\n",
    "                    cv2.imwrite(frame_path, frame)\n",
    "                    unique_hashes.append(frame_hash)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if multi_bird_detected:\n",
    "        print(f\"There is more than one bird in the video {video_path.name}, better choose another one!\")\n",
    "    else:\n",
    "        print(f\"Processed video {video_path.name}\")\n",
    "\n",
    "\n",
    "video_files = list(Path(INPUT_DIR).glob(\"*.MP4\"))\n",
    "if not video_files:\n",
    "    print(\"No videos found in the input directory.\")\n",
    "else:\n",
    "    for video in tqdm(video_files, desc=\"Processing videos\"):\n",
    "        process_video(video)\n",
    "\n",
    "print(f\"Extracted frames saved in: {OUTPUT_DIR}\")\n",
    "\n",
    "# Mask Processing\n",
    "\n",
    "input_dir = \"Extracted_frames\"   # Directory holding extracted frames\n",
    "yolo_model = YOLO(\"yolo11x-seg.pt\")  # Reload the YOLO segmentation model\n",
    "\n",
    "MIN_BLOB_PIXELS = 5000\n",
    "BOTTOM_FRACTION_ROW = 1 / 4\n",
    "BOTTOM_FRACTION_NARROW = 1 / 2\n",
    "HORIZONTAL_THRESHOLD = 0.8\n",
    "NARROW_SEGMENT_THRESHOLD = 100\n",
    "\n",
    "def filter_horizontal_rows(mask, threshold, bottom_fraction):\n",
    "    \"\"\"\n",
    "    Filters out horizontal rows near the bottom of the image if\n",
    "    the fraction of black pixels is above a certain threshold\n",
    "    \"\"\"\n",
    "    start_row = int(mask.shape[0] * (1 - bottom_fraction))\n",
    "    for row_idx in range(start_row, mask.shape[0]):\n",
    "        row = mask[row_idx, :]\n",
    "        # If the row is mostly black (above threshold), clear it\n",
    "        if 1 - (np.sum(row) / row.shape[0]) >= threshold:\n",
    "            mask[row_idx, :] = 0\n",
    "    return mask\n",
    "\n",
    "def filter_narrow_segments(mask, max_width, bottom_fraction):\n",
    "    \"\"\"\n",
    "    Filters out narrow segments of white pixels in each row\n",
    "    near the bottom of the image if their width is under a threshold\n",
    "    \"\"\"\n",
    "    start_row = int(mask.shape[0] * (1 - bottom_fraction))\n",
    "    for row_idx in range(start_row, mask.shape[0]):\n",
    "        row_indices = np.where(mask[row_idx])[0]\n",
    "        segments = np.split(row_indices, np.where(np.diff(row_indices) > 1)[0] + 1)\n",
    "        for segment in segments:\n",
    "            if len(segment) <= max_width:\n",
    "                mask[row_idx, segment] = 0\n",
    "    return mask\n",
    "\n",
    "def remove_small_blobs(mask, min_pixels):\n",
    "    \"\"\"\n",
    "    Removes connected blobs smaller than a certain pixel count.\n",
    "    \"\"\"\n",
    "    labeled_mask, num_features = label(mask)\n",
    "    valid_labels = [\n",
    "        i for i in range(1, num_features + 1)\n",
    "        if np.sum(labeled_mask == i) >= min_pixels\n",
    "    ]\n",
    "    return np.isin(labeled_mask, valid_labels)\n",
    "\n",
    "for frame_name in tqdm(os.listdir(input_dir), desc=\"Frames\"):\n",
    "    frame_path = os.path.join(input_dir, frame_name)\n",
    "    if not frame_name.lower().endswith('.png'):\n",
    "        continue\n",
    "\n",
    "    # Run YOLO detection on the full frame\n",
    "    results = yolo_model.predict(frame_path, conf=0.4, verbose=False)\n",
    "    if not results[0].boxes:\n",
    "        os.remove(frame_path)\n",
    "        continue\n",
    "\n",
    "    # Crop region of interest using the highest-confidence box\n",
    "    img = cv2.imread(frame_path)\n",
    "    x1, y1, x2, y2 = map(int, results[0].boxes.xyxy[0])\n",
    "    cropped = img[y1:y2, x1:x2]\n",
    "\n",
    "    # Predict segmentation mask on cropped region\n",
    "    mask_results = yolo_model.predict(cropped, conf=0.6, verbose=False)\n",
    "    if not mask_results[0].masks:\n",
    "        os.remove(frame_path)\n",
    "        continue\n",
    "\n",
    "    mask = mask_results[0].masks.data[0].cpu().numpy().astype(bool)\n",
    "\n",
    "    # Apply mask filters\n",
    "    mask = filter_horizontal_rows(mask, HORIZONTAL_THRESHOLD, BOTTOM_FRACTION_ROW)\n",
    "    mask = filter_narrow_segments(mask, NARROW_SEGMENT_THRESHOLD, BOTTOM_FRACTION_NARROW)\n",
    "    mask = remove_small_blobs(mask, MIN_BLOB_PIXELS)\n",
    "\n",
    "    # If there is no mask remaining, discard the frame\n",
    "    if not np.any(mask):\n",
    "        os.remove(frame_path)\n",
    "        continue\n",
    "\n",
    "    # Resize mask if needed\n",
    "    if mask.shape[:2] != cropped.shape[:2]:\n",
    "        mask = cv2.resize(\n",
    "            mask.astype(np.uint8),\n",
    "            (cropped.shape[1], cropped.shape[0]),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        ).astype(bool)\n",
    "\n",
    "    # Apply mask to the cropped image\n",
    "    mask_rgb = np.zeros_like(cropped)\n",
    "    mask_rgb[mask] = cropped[mask]\n",
    "    cv2.imwrite(frame_path, mask_rgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following approach selects the highest softmax probability and records it for each frame. AKA it treats every frame’s “winner” prediction equally, even if some predictions were made with low confidence or if the probability distribution was very flat (when the highest predicted probability is only marginally greater than the probabilities for other classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 42/42 [00:09<00:00,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Results:\n",
      "B18: 78.57%\n",
      "B29: 11.90%\n",
      "B04: 7.14%\n",
      "B52: 2.38%\n",
      "\n",
      "The bird is most likely B18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_PATH = \"./best_model.pth\"\n",
    "FRAME_DIR = \"Extracted_frames\"\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'B02', 'B03', 'B04', 'B05', 'B07', 'B11', 'B18', 'B23',\n",
    "    'B26', 'B29', 'B30', 'B31', 'B47', 'B49', 'B50', 'B52'\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet50(pretrained=False, num_classes=16)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(CLASS_NAMES))\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "predictions = []\n",
    "for frame_name in tqdm(os.listdir(FRAME_DIR), desc=\"Processing frames\"):\n",
    "    frame_path = os.path.join(FRAME_DIR, frame_name)\n",
    "    if not frame_name.lower().endswith('.png'):\n",
    "        continue\n",
    "\n",
    "    image = Image.open(frame_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "        predicted_class = probabilities.argmax().item()\n",
    "\n",
    "        if predicted_class >= len(CLASS_NAMES):\n",
    "            print(f\"Warning: Predicted class index {predicted_class} is out of bounds for CLASS_NAMES.\")\n",
    "            continue\n",
    "\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "# Count how many times each class was predicted\n",
    "prediction_counts = Counter(predictions)\n",
    "total_predictions = sum(prediction_counts.values())\n",
    "\n",
    "if total_predictions == 0:\n",
    "    print(\"No valid predictions were made.\")\n",
    "    exit()\n",
    "\n",
    "# Calculate percentage for each class\n",
    "percentages = {\n",
    "    CLASS_NAMES[k]: (v / total_predictions) * 100\n",
    "    for k, v in prediction_counts.items()\n",
    "}\n",
    "sorted_percentages = dict(\n",
    "    sorted(percentages.items(), key=lambda item: item[1], reverse=True)\n",
    ")\n",
    "\n",
    "# Determine the most common class\n",
    "most_common_class = max(prediction_counts, key=prediction_counts.get)\n",
    "most_common_percentage = sorted_percentages[CLASS_NAMES[most_common_class]]\n",
    "\n",
    "print(\"\\nPrediction Results:\")\n",
    "for bird_id, percentage in sorted_percentages.items():\n",
    "    print(f\"{bird_id}: {percentage:.2f}%\")\n",
    "\n",
    "print(f\"\\nThe bird is most likely {CLASS_NAMES[most_common_class]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 42/42 [00:09<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Results (ArgMax Only):\n",
      "B52: 50.00%\n",
      "B26: 33.33%\n",
      "B18: 16.67%\n",
      "\n",
      "The bird is most likely B52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reproducibility settings\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Force deterministic algorithms in cuDNN (Ensuring the approach is deterministic. Otherwise re running the model was outputting a different result)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Classification (ArgMax only)\n",
    "\n",
    "MODEL_PATH = \"./best_model.pth\"\n",
    "FRAME_DIR = \"Extracted_frames\"\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'B02', 'B03', 'B04', 'B05', 'B07', 'B11', 'B18', 'B23',\n",
    "    'B26', 'B29', 'B30', 'B31', 'B47', 'B49', 'B50', 'B52'\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model \n",
    "model = models.resnet50(pretrained=False, num_classes=16)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(CLASS_NAMES))\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for frame_name in tqdm(os.listdir(FRAME_DIR), desc=\"Processing frames\"):\n",
    "    frame_path = os.path.join(FRAME_DIR, frame_name)\n",
    "    if not frame_name.lower().endswith('.png'):\n",
    "        continue\n",
    "\n",
    "    image = Image.open(frame_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        predicted_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "        if predicted_class >= len(CLASS_NAMES):\n",
    "            print(f\"Warning: Predicted class index {predicted_class} is out of bounds for CLASS_NAMES.\")\n",
    "            continue\n",
    "\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "# count how many times each class was predicted\n",
    "prediction_counts = Counter(predictions)\n",
    "total_predictions = sum(prediction_counts.values())\n",
    "\n",
    "if total_predictions == 0:\n",
    "    print(\"No valid predictions were made.\")\n",
    "    exit()\n",
    "\n",
    "# calculate percentage for each class\n",
    "percentages = {\n",
    "    CLASS_NAMES[k]: (v / total_predictions) * 100\n",
    "    for k, v in prediction_counts.items()\n",
    "}\n",
    "sorted_percentages = dict(\n",
    "    sorted(percentages.items(), key=lambda item: item[1], reverse=True)\n",
    ")\n",
    "\n",
    "# determine the most common class\n",
    "most_common_class = max(prediction_counts, key=prediction_counts.get)\n",
    "\n",
    "print(\"\\nPrediction Results (ArgMax Only):\")\n",
    "for bird_id, percentage in sorted_percentages.items():\n",
    "    print(f\"{bird_id}: {percentage:.2f}%\")\n",
    "\n",
    "print(f\"\\nThe bird is most likely {CLASS_NAMES[most_common_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 42/42 [00:09<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Frame Predictions:\n",
      "Frame               B02       B03       B04       B05       B07       B11       B18       B23       B26       B29       B30       B31       B47       B49       B50       B52       \n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "B18_20181201_056_frame840.png    5.84%     8.71%     4.84%     5.37%     5.00%     4.25%    10.21%     4.54%    11.86%     3.36%     3.99%     4.35%     4.45%     3.10%     6.73%    13.40% \n",
      "B18_20181113_089_frame1080.png    6.04%     5.81%     3.63%     3.70%     7.83%     3.14%     9.75%     3.85%    10.14%     3.63%     8.49%     4.22%     4.59%     3.15%     6.66%    15.37% \n",
      "B18_20181208_005_frame1500.png    9.72%     3.20%     4.18%     3.91%     5.82%     2.59%    10.47%     3.48%    14.37%     3.77%     6.02%     4.42%     4.51%     4.69%     7.14%    11.70% \n",
      "B18_20181208_005_frame180.png    7.34%     6.64%     4.25%     3.88%     6.18%     2.29%    11.16%     1.94%    16.44%     2.34%     5.06%     3.57%     4.22%     2.36%     6.34%    15.98% \n",
      "B18_20181201_056_frame660.png    6.74%     6.58%     4.96%     4.86%     4.66%     4.88%     9.75%     3.55%    10.78%     3.73%     6.28%     4.64%     5.80%     3.06%     6.72%    13.00% \n",
      "B18_20181208_005_frame960.png    5.70%     5.51%     4.89%     4.30%     6.67%     3.22%    13.75%     3.56%    16.40%     3.21%     3.95%     4.25%     3.65%     3.86%     6.63%    10.46% \n",
      "B18_20181201_056_frame360.png    6.12%     8.46%     5.93%     4.15%     5.66%     4.23%    10.04%     3.96%    10.86%     3.04%     3.70%     4.61%     5.17%     3.20%     6.68%    14.20% \n",
      "B18_20181208_005_frame720.png    4.98%     5.30%     4.59%     5.78%     8.38%     3.92%    12.22%     5.18%    12.83%     3.40%     5.06%     2.42%     4.19%     6.37%     5.27%    10.12% \n",
      "B18_20181113_089_frame1020.png    7.40%     5.27%     3.32%     3.31%     5.38%     3.66%    11.02%     4.07%    13.15%     2.36%     5.61%     3.92%     4.39%     2.63%     7.42%    17.10% \n",
      "B18_20181113_089_frame300.png    7.51%     5.51%     3.90%     3.52%     5.47%     3.55%    12.19%     3.02%    11.58%     4.07%     6.60%     4.23%     4.23%     3.78%     7.52%    13.32% \n",
      "B18_20181113_089_frame840.png    6.15%     4.81%     4.23%     4.20%     6.66%     4.07%     9.19%     2.93%    13.17%     3.11%     5.24%     4.78%     4.01%     2.37%     7.59%    17.47% \n",
      "B18_20181208_005_frame1560.png    8.00%     5.51%     4.68%     4.53%     3.20%     3.89%    12.76%     5.29%    10.26%     3.04%     6.94%     4.71%     5.25%     5.87%     7.32%     8.75% \n",
      "B18_20181208_005_frame120.png    6.46%     7.58%     4.87%     4.49%     6.60%     3.04%    10.58%     3.45%    12.36%     3.80%     6.70%     4.03%     4.36%     3.04%     5.45%    13.18% \n",
      "B18_20181113_089_frame60.png    6.95%     6.60%     3.91%     3.79%     5.24%     3.18%    12.87%     3.89%    14.52%     3.23%     5.99%     3.74%     4.08%     3.00%     5.61%    13.42% \n",
      "B18_20181208_005_frame900.png    5.12%     5.43%     5.01%     4.80%     9.18%     3.08%    14.03%     5.70%    13.60%     3.01%     4.37%     2.16%     4.28%     5.09%     6.12%     9.01% \n",
      "B18_20181208_005_frame240.png    7.38%     5.65%     4.16%     4.50%     5.97%     1.64%    12.76%     2.82%    15.58%     2.83%     7.83%     2.57%     5.98%     3.47%     6.47%    10.39% \n",
      "B18_20181210_007_frame240.png    6.48%     7.65%     5.29%     4.31%     5.62%     3.21%    11.73%     3.02%    16.71%     2.32%     3.72%     3.97%     4.24%     3.10%     6.09%    12.55% \n",
      "B18_20181113_089_frame240.png    6.97%     5.48%     4.43%     3.55%     4.91%     3.23%    10.79%     4.07%    13.88%     3.37%     5.89%     4.55%     4.18%     2.69%     6.17%    15.83% \n",
      "B18_20181114_005_frame120.png    6.73%     5.63%     4.30%     3.87%     6.88%     2.85%    10.83%     2.28%    14.36%     2.05%     5.39%     3.67%     4.74%     2.48%     6.26%    17.68% \n",
      "B18_20181208_005_frame660.png    5.07%     5.97%     4.49%     6.96%     6.07%     3.45%    10.39%     4.48%    17.34%     2.50%     4.35%     3.06%     4.66%     4.29%     7.11%     9.80% \n",
      "B18_20181208_005_frame1380.png    6.68%     6.24%     4.19%     6.07%     6.92%     2.57%    11.08%     3.08%    10.51%     5.26%     5.56%     5.14%     3.45%     3.70%     6.41%    13.12% \n",
      "B18_20181208_005_frame840.png    5.67%     5.29%     3.51%     4.38%     8.02%     3.00%    16.51%     5.90%    14.95%     1.57%     2.78%     1.46%     4.72%     5.18%     6.80%    10.25% \n",
      "B18_20181201_056_frame780.png    6.90%     7.55%     4.40%     5.54%     5.33%     3.59%    11.06%     4.14%    10.01%     4.46%     6.48%     4.72%     5.28%     4.00%     6.50%    10.02% \n",
      "B18_20181208_005_frame1020.png    5.06%     8.79%     5.32%     4.79%     8.29%     2.65%    14.17%     5.04%    14.69%     2.64%     3.43%     2.79%     4.46%     3.74%     5.62%     8.51% \n",
      "B18_20181208_005_frame1140.png    4.86%     7.75%     5.48%     5.38%     8.41%     2.83%    14.50%     5.70%    12.82%     3.08%     3.68%     2.80%     4.66%     4.53%     5.08%     8.45% \n",
      "B18_20181201_056_frame420.png    6.18%     8.01%     5.11%     5.53%     5.37%     3.93%     7.06%     3.60%    11.42%     2.71%     4.08%     6.55%     5.05%     3.90%     8.81%    12.70% \n",
      "B18_20181208_005_frame300.png    5.60%     7.23%     4.44%     3.70%     6.46%     2.00%    13.76%     2.43%    14.37%     2.83%     7.08%     2.96%     5.09%     2.72%     5.80%    13.52% \n",
      "B18_20181113_089_frame480.png    6.17%     6.18%     4.61%     3.87%     8.34%     3.69%    11.98%     4.13%    11.27%     3.44%     5.28%     4.03%     4.32%     3.44%     6.51%    12.76% \n",
      "B18_20181113_089_frame720.png    4.92%     5.18%     6.00%     6.02%     7.71%     3.73%     8.84%     4.48%    11.54%     4.21%     7.76%     4.00%     4.16%     4.51%     6.73%    10.20% \n",
      "B18_20181208_005_frame1440.png    7.16%     4.38%     4.60%     5.43%     5.90%     2.51%    11.93%     4.82%    12.65%     3.50%     6.74%     4.15%     5.43%     5.31%     5.55%     9.94% \n",
      "B18_20181113_089_frame960.png    6.63%     5.13%     3.97%     3.41%     6.94%     4.69%    10.49%     3.79%    10.13%     3.19%     6.13%     4.66%     4.85%     3.21%     6.94%    15.83% \n",
      "B18_20181114_005_frame180.png    6.73%     8.80%     4.28%     3.18%     6.47%     2.43%    12.55%     2.62%    10.17%     3.20%     4.84%     5.14%     4.16%     2.35%     6.39%    16.69% \n",
      "B18_20181113_089_frame180.png    7.30%     7.06%     3.67%     4.87%     7.57%     4.41%    10.02%     4.65%    10.28%     4.01%     6.27%     4.20%     4.09%     3.76%     5.58%    12.25% \n",
      "B18_20181208_005_frame600.png    5.14%     5.45%     4.25%     5.98%     6.41%     3.24%    13.84%     4.59%    13.40%     3.32%     4.77%     3.02%     5.33%     4.57%     6.74%     9.95% \n",
      "B18_20181208_005_frame1320.png    6.83%     4.74%     5.01%     4.11%     5.68%     2.63%    11.73%     3.40%    12.30%     4.30%     6.60%     4.55%     3.62%     3.08%     6.74%    14.67% \n",
      "B18_20181201_056_frame720.png    6.48%     9.06%     3.36%     5.16%     5.44%     3.93%     6.76%     6.56%     7.41%     3.81%     4.79%     6.91%     6.45%     5.06%     5.72%    13.07% \n",
      "B18_20181114_005_frame420.png    6.06%     5.57%     4.61%     5.55%     7.12%     2.12%    16.52%     4.24%    14.86%     2.63%     3.31%     2.89%     4.22%     4.97%     6.30%     9.01% \n",
      "B18_20181208_005_frame1080.png    5.85%     7.23%     5.15%     6.47%     8.25%     3.76%    10.31%     5.36%    12.65%     3.01%     4.85%     2.98%     4.08%     4.83%     5.26%     9.96% \n",
      "B18_20181201_056_frame480.png    5.56%     9.01%     5.77%     4.23%     5.53%     3.72%    10.32%     3.40%    11.88%     3.12%     3.23%     5.21%     3.90%     3.12%     7.01%    14.99% \n",
      "B18_20181113_089_frame540.png    6.59%     4.94%     3.87%     3.37%     7.33%     2.22%    13.00%     3.61%    16.57%     2.89%     6.82%     3.32%     4.38%     3.16%     5.41%    12.52% \n",
      "B18_20181113_089_frame420.png    7.11%     5.95%     3.55%     4.63%     8.05%     2.90%    10.58%     3.21%    12.36%     3.39%     6.99%     4.21%     4.03%     2.89%     5.55%    14.59% \n",
      "B18_20181113_089_frame780.png    7.43%     4.31%     3.48%     2.51%     5.88%     3.24%     9.95%     2.58%    13.02%     2.77%     4.81%     4.64%     4.06%     2.38%     8.81%    20.15% \n",
      "\n",
      "Prediction Results:\n",
      "B52: 50.00%\n",
      "B26: 33.33%\n",
      "B18: 16.67%\n",
      "\n",
      "The bird is most likely B52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reproducibility settings\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Force cuDNN to be deterministic\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Classification\n",
    "\n",
    "MODEL_PATH = \"./best_model.pth\"\n",
    "FRAME_DIR = \"Extracted_frames\"\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'B02', 'B03', 'B04', 'B05', 'B07', 'B11', 'B18', 'B23',\n",
    "    'B26', 'B29', 'B30', 'B31', 'B47', 'B49', 'B50', 'B52'\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.resnet50(pretrained=False, num_classes=16)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, len(CLASS_NAMES))\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "predictions = []\n",
    "frame_results = []\n",
    "\n",
    "for frame_name in tqdm(os.listdir(FRAME_DIR), desc=\"Processing frames\"):\n",
    "    frame_path = os.path.join(FRAME_DIR, frame_name)\n",
    "    if not frame_name.lower().endswith('.png'):\n",
    "        continue\n",
    "\n",
    "    image = Image.open(frame_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "        predicted_class = probabilities.argmax().item()\n",
    "\n",
    "        # Save frame name and probability distribution for later printing\n",
    "        frame_results.append((frame_name, probabilities.cpu().tolist()))\n",
    "\n",
    "        if predicted_class >= len(CLASS_NAMES):\n",
    "            print(f\"Warning: Predicted class index {predicted_class} is out of bounds for CLASS_NAMES.\")\n",
    "            continue\n",
    "\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "header = \"Frame\".ljust(20) + \"\".join(name.ljust(10) for name in CLASS_NAMES)\n",
    "print(\"\\nDetailed Frame Predictions:\")\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# Print each frame's probabilities\n",
    "for frame_name, probs in frame_results:\n",
    "    row = frame_name.ljust(20)\n",
    "    for p in probs:\n",
    "        row += f\"{p*100:8.2f}% \"\n",
    "    print(row)\n",
    "\n",
    "# Original majority voting\n",
    "prediction_counts = Counter(predictions)\n",
    "total_predictions = sum(prediction_counts.values())\n",
    "\n",
    "if total_predictions == 0:\n",
    "    print(\"No valid predictions were made.\")\n",
    "    exit()\n",
    "\n",
    "# Calculate percentage for each class from majority votes\n",
    "percentages = {\n",
    "    CLASS_NAMES[k]: (v / total_predictions) * 100\n",
    "    for k, v in prediction_counts.items()\n",
    "}\n",
    "sorted_percentages = dict(\n",
    "    sorted(percentages.items(), key=lambda item: item[1], reverse=True)\n",
    ")\n",
    "\n",
    "most_common_class = max(prediction_counts, key=prediction_counts.get)\n",
    "\n",
    "print(\"\\nPrediction Results:\")\n",
    "for bird_id, percentage in sorted_percentages.items():\n",
    "    print(f\"{bird_id}: {percentage:.2f}%\")\n",
    "\n",
    "print(f\"\\nThe bird is most likely {CLASS_NAMES[most_common_class]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bower_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
